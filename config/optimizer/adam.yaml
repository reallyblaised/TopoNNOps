# # Adam optimizer configuration
# optimizer:
#   name: "adam"
#   params:
#     lr: 1e-3
#     weight_decay: 1e-4
#     beta1: 0.9
#     beta2: 0.999
#     eps: 1e-8
#     amsgrad: false
    
#   # Learning rate scheduler options
#   scheduler:
#     name: "reduce_on_plateau"
#     params:
#       mode: "min"
#       factor: 0.1
#       patience: 5
#       threshold: 1e-4
#       threshold_mode: "rel"
#       cooldown: 0
#       min_lr: 1e-6
#       eps: 1e-8
#       verbose: true
    
#     # Alternative scheduler configurations
#     alternatives:
#       cosine:
#         T_max: 50
#         eta_min: 0
#         last_epoch: -1
      
#       step:
#         step_size: 10
#         gamma: 0.1
#         last_epoch: -1
      
#       one_cycle:
#         max_lr: 1e-2
#         epochs: 50
#         steps_per_epoch: null  # Will be set based on data
#         pct_start: 0.3
#         anneal_strategy: "cos"
        
#   # Gradient clipping options
#   gradient_clipping:
#     enabled: true
#     max_norm: 1.0
#     norm_type: 2.0
    
#   # Learning rate warmup
#   warmup:
#     enabled: true
#     type: "linear"  # or "exponential"
#     steps: 100
#     ratio: 0.1

# simple optimizer config
# -----------------------
# config/optimizer/adam.yaml - Simple optimizer configuration
name: "adam"
lr: 1e-3
weight_decay: 0.0